{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ZgHqTg40559AnfWYGa0iUrqhDOe-D11H",
      "authorship_tag": "ABX9TyPmXabJ5ItutA4O5Lwo6WAM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrisjmccormick/summarize-long-pdfs/blob/main/Summarizing_Long_PDFs_with_ChatGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#▂▂▂▂▂▂▂▂▂▂▂▂"
      ],
      "metadata": {
        "id": "ufdSa_moo_CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Introduction"
      ],
      "metadata": {
        "id": "tkKS_SSdR6mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/chrisjmccormick/summarize-long-pdfs/)\n",
        "\n",
        "A friend of mine was taking a college course in political science with a ton of assigned reading material, and found that ChatGPT could produce helpful summaries (and in case you're wondering, the summaries are intended as an additional learning aid, rather than a replacement for doing the reading 😜).\n",
        "\n",
        "There were a few challenges to trying to use ChatGPT for this, though:\n",
        "\n",
        "* The reading materials are in the form of PDFs, and there are just too many (39! 😳) to do this manually.\n",
        "* Most of the readings are too long to fit into ChatGPT in a single pass.\n",
        "* Some of the PDFs are scans (or even just photos!) of pages from books, and none of the text is selectable.\n",
        "* Even for the PDFs which do have selectable text, copying and pasting it into ChatGPT isn't trivial.\n",
        "\n",
        "So, I created this Notebook to automate the process and summarize all _39_ of the PDFs assigned for the class, and it sounds like they were really helpful!\n",
        "\n",
        "This Notebook is intended both as a relatively polished tool for completing this task, and as a tutorial and example code for working on this \"summarization\" problem yourself. I'm sure you can improve on it by experimenting with various details of the process!\n",
        "\n",
        "_Note: I think the biggest caveat to this Notebook as a practical tool is that it_ does _rely on OpenAI's interface, which means you'll need to do some setup work on OpenAI's website in order to fully run it. Sorry!_"
      ],
      "metadata": {
        "id": "facbpNPoPNmz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## i. Text Sources"
      ],
      "metadata": {
        "id": "CAcgdz0f8XM6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 1** of this Notebook turns all of the PDFs into \"**plain text**\" .txt files. The `PyMuPDF` library has everything we need for this--it can extract text from the PDFs that have it, and can run \"OCR\" (optical character recognition) for PDFs that only contain text in the form of images. For text extraction, it uses the `tesseract` library.\n"
      ],
      "metadata": {
        "id": "-If4zH8gzuYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**PDFs, eBooks, and Physical Books**\n",
        "\n",
        "I think it's worth pointing out that you can use this same code to summarize portions of text from literally any source--including eBooks and paper books--you'll just have to create PDFs first.\n",
        "\n",
        "For an **eBook**, you could paste **screenshots** (since eBook readers don't allow you to copy text) into a Google Doc and then save it as a PDF. I imagine text extraction should work great on such clean images.\n",
        "\n",
        "For a **paper book**, you could use a scanner app on your phone (I often use the scan tool in the \"Files\" app on my iPhone).\n",
        "\n",
        "If you already have the **plain text** you want summarized, then you can just place it in a `.txt` file with the suffix `* - Parsed.txt` and the summarization code will work on it.\n",
        "\n",
        "For something on the **web**, you may just be able to copy and paste the text. But if that's tricky, then, hey, you can always save it as a PDF! 😊\n",
        "\n",
        "(Note: I also included some starter code in the appendix for extracting text straight from the web page's HTML instead.)\n"
      ],
      "metadata": {
        "id": "rX5IDe7v8k8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ii. Using ChatGPT"
      ],
      "metadata": {
        "id": "T1uU0HA98wsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Part 2** of the Notebook uses ChatGPT to **summarize** each of those .txt files.\n",
        "\n"
      ],
      "metadata": {
        "id": "Uq5Avs-8TkS_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Garbled Text**\n",
        "\n",
        "I think something great here about the way ChatGPT works is that, similar to you and I, it is pretty good at making sense of imperfect text. The text that comes out of the PDFs can be pretty messy, especially if the document contains tables and figures, and yet GPT still seems to perform great!\n"
      ],
      "metadata": {
        "id": "OFD4EiC4YWNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Length Limit**\n",
        "\n",
        "One of the challenges here is that ChatGPT can't consume a 25-page book chapter all at once. There's a limit to how much text you can give it.\n",
        "\n",
        "You _can_ break that book chapter into, e.g., 4 separate chunks that are each just within the limit. There's still a problem, though... When you feed ChatGPT a big chunk of text like this, it actually has no memory of the previous chunks you've given it!\n",
        "\n",
        "The way ChatGPT works is that every time it replies to you, it actually _re-reads your entire chat history_, plus your latest message, in order to respond to you. This creates the _illusion_ that it remembers what you've been talking about. In reality, once your conversation goes beyond that length limit, older messages get dropped, and it has _zero_ knowledge of them.\n",
        "\n",
        "The implication of this is that your **entire chat history**, plus your **next prompt**, and even its own **reply**, all have to fit within the text-length limit.\n",
        "\n",
        "So we'll have to get creative in how we work around this limitation!\n"
      ],
      "metadata": {
        "id": "2rcKA53VYXiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarizing Across Chunks**\n",
        "\n",
        "The way I chose to address this was to give GPT the summary so far along with the next chunk to summarize. Then I give it all of the chunk summaries and ask it to create a single more concise summary.\n",
        "\n",
        "You'll see the exact process and my wording (i.e., my \"prompt\") down in Part 2!\n",
        "\n",
        "There's another challenge here around figuring out _where_ exactly to split the text in order to get chunks of the appropriate size. I did this using OpenAI's \"tokenizer\", and I'll get into the details of that as well.\n"
      ],
      "metadata": {
        "id": "EhD0AJG-WMtz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using OpenAI**\n",
        "\n",
        "It's possible to use what I've created here for free by going to chat.openai.com and copying and pasting things in and out of their chat interface...\n",
        "\n",
        "To make this all _automatic_, though, this Notebook is set up to use **OpenAI's web service**. In order to run it, you'll need to register for an OpenAI account and grab your **\"API key\"** to plug into this Notebook further down.\n",
        "\n",
        "It's pretty easy to do, and I assume they still provide some **free credits** when you start out. If you're using this _heavily_ you may eventually need to pay a little bit, but the free credits should take you a long way.\n",
        "\n",
        "Kind of a bummer, I know. Sorry!"
      ],
      "metadata": {
        "id": "gh8hRpweXGK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "-------\n",
        "\n",
        "Aside: *Aren't there smaller models now that you can run yourself for free with similar performance to GPT?*\n",
        "\n",
        "Yes! These are exciting.\n",
        "\n",
        "But... I think the catch there is that using your own GPU, or a free one on Colab, will only work for _relatively short_ inputs.\n",
        "\n",
        "The problem is that the amount of memory and compute required by GPT grows _exponentially_ with the length of the text!\n",
        "\n",
        "Because of this, I don't think you're going to be able to use the maximum 4,096 tokens that GPT can handle, and I'm worried that this won't work as well if you have to break the article into too small of chunks.\n",
        "\n",
        "But I could be wrong on both counts, honestly! Let me know if you try it. 😊\n",
        "\n",
        "--------\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hyECwbRaTgax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#▂▂▂▂▂▂▂▂▂▂▂▂"
      ],
      "metadata": {
        "id": "8iUKmzgMR_NU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Extracting Text from PDFs"
      ],
      "metadata": {
        "id": "NevQkB0wo_CR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.1. Install PDF Libraries"
      ],
      "metadata": {
        "id": "njOD5JB9pHwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tesseract**\n",
        "\n",
        "Tesseract is a library for extracting text from images. PyMuPDF requires it for this functionality.\n",
        "\n",
        "Note that this is not a Python library (though I'm sure some Python wrappers exist out there), so we're installing it with `apt`."
      ],
      "metadata": {
        "id": "WSCxECVnQHHN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tesseract-ocr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFzYyBa4D7hN",
        "outputId": "5b0836aa-0020-45ff-c59f-e1e53d1f1349"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 30 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It should install to following folder:"
      ],
      "metadata": {
        "id": "7Bsm9Hn8QoHO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /usr/share/tesseract-ocr/4.00/tessdata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYLVj77YEESQ",
        "outputId": "46fd2769-3092-41e7-ca80-6460418dbe4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "configs  eng.traineddata  osd.traineddata  pdf.ttf  tessconfigs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyMuPDF needs this environment variable set in order to locate tesseract."
      ],
      "metadata": {
        "id": "E_fBmhfjj3vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TESSDATA_PREFIX\"] = \"/usr/share/tesseract-ocr/4.00/tessdata\""
      ],
      "metadata": {
        "id": "trcK7W38j1_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PyMuPDF**\n",
        "\n",
        "The `PyMuPDF` (on GitHub [here](https://github.com/pymupdf/PyMuPDF), docs [here](https://pymupdf.readthedocs.io/en/latest/)) library is actually written in C, and this is just a Python wrapper for it. The logo for the library is written with the \"mu\" character, \"μ\", so perhaps the name is actually \"Micro PDF\"? 🤷‍♂️"
      ],
      "metadata": {
        "id": "XyolZNnzkoFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I'm specifying this version number because, as of today (8/31/23), version\n",
        "# 1.23 is telling me that OCR isn't supported.\n",
        "!pip install pymupdf==1.22.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koVV-owJCVlu",
        "outputId": "3662c40a-0b6b-493f-8eb3-478e6ac58f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymupdf==1.22.5 in /usr/local/lib/python3.10/dist-packages (1.22.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that when importing PyMuPDF, it seems the module is actually named `fitz`, for whatever reason.\n",
        "\n",
        "This code from their GitHub documentation verifies the library version and sets an environment variable to indicate where Tesseract can be found."
      ],
      "metadata": {
        "id": "U6l4He2ulz1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "\n",
        "print('PyMuPDF version', fitz.VersionBind)\n",
        "if tuple(map(int, fitz.VersionBind.split(\".\"))) < (1, 19, 1):\n",
        "    raise ValueError(\"Need at least v1.19.1 of PyMuPDF\")"
      ],
      "metadata": {
        "id": "RtJRZrKDltIQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25317029-71db-457b-88c0-05b76ac8b909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyMuPDF version 1.22.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, define a little helper function for formatting the elapsed time. (The OCR process can be kinda slow!)"
      ],
      "metadata": {
        "id": "bHnXUyy7nAsU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "nXptltSxnC-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.2. Path to PDFs"
      ],
      "metadata": {
        "id": "AgOpmkXhiwA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I put all of the PDFs onto my Google Drive under a folder named `Readings`.\n",
        "Make sure to update this path to point to your folder.\n",
        "\n",
        "If you just want to try this out and don't have your own PDFs yet, here are links to the examples I'm using: [1](https://drive.google.com/file/d/1RZ2iNDaxSxgcpqdXhAY4huT5FRD1Rpbq/view?usp=sharing), [2](https://drive.google.com/file/d/1FBpnhr1k_mIrDUYC-XmqwwxAzKdfO9YN/view?usp=sharing), [3](https://drive.google.com/file/d/11JSMd93t4TxNjxekEwOvlIH_-pncBsMW/view?usp=sharing)"
      ],
      "metadata": {
        "id": "6dMS-zAwjEEA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"./drive/MyDrive/Readings/\""
      ],
      "metadata": {
        "id": "ugZ2J9OXOwZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell just gets a list of all of the PDFs in the specified folder."
      ],
      "metadata": {
        "id": "NfPsggqXOydY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "files = os.listdir(dir)\n",
        "\n",
        "# We'll construct a list of just paths to the PDFs.\n",
        "pdf_filenames = []\n",
        "\n",
        "# For each file in the directory...\n",
        "for f in files:\n",
        "\n",
        "    # Filter for pdfs.\n",
        "    if '.pdf' in f:\n",
        "\n",
        "        # Add to the list\n",
        "        pdf_filenames.append(f)\n",
        "\n",
        "        print(f)"
      ],
      "metadata": {
        "id": "fyYh8nzVjfG9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72dabc9f-22e5-48e5-d501-b2d0fa111200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "America and China Cooperating on Climate.pdf\n",
            "Machiavelli - The Prince - Chp 18.pdf\n",
            "Machiavelli - The Prince - Chp 19.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.3. Extract the Text!"
      ],
      "metadata": {
        "id": "gUggHaXeedMi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll loop through the PDFs and parse each of them!\n",
        "\n"
      ],
      "metadata": {
        "id": "HyThlgjLeZ-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Output Files**\n",
        "\n",
        "After parsing a PDF, we'll write the plain text out to a `.txt` file. It will have the same filename as the PDF, but with the extension ` - Parsed.txt`.\n",
        "\n",
        "For example, \"America and China Cooperating on Climate - Parsed.txt\"\n",
        "\n",
        "Part 2 of this Notebook, which performs the summarization, runs on any \"* - Parsed.txt\" files it finds in the directory.\n",
        "\n"
      ],
      "metadata": {
        "id": "AfTNHayyPqWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text vs. OCR**\n",
        "\n",
        "I'm using an overly simple approach (which _usually_ worked fine) to determining whether we need to run OCR or not.\n",
        "\n",
        "I use the library's `get_text()` function (which does _not_ perform OCR) on the PDF, and if it returns anything, then I assume this is a text PDF that doesn't require OCR.  \n",
        "\n",
        "If `get_text` returns an empty string, though, then I run OCR on it.\n",
        "\n",
        "> Note: I did come across one PDF which was image based, but where the `get_text` function did return a tiny bit of text--I think it was returning page titles and page numbers for each page.\n",
        "> I'm not sure of a good general solution to this. I just tweaked my copy of the code to handle that file specially. 🤷‍♂️\n",
        "\n",
        "The OCR step can be slow, so I also included a check to skip over files that've already been parsed (by checking for the existence of the `'* - Parsed.txt'` file)."
      ],
      "metadata": {
        "id": "9XW3Yi9hQG7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# For each of the documents...\n",
        "for (pdf_num, pdf_name) in enumerate(pdf_filenames):\n",
        "\n",
        "\n",
        "    # Print out which one we're on.\n",
        "    print('\\n======== {:} ({:} of {:}) ========\\n'.format(pdf_name, pdf_num + 1, len(pdf_filenames)) )\n",
        "\n",
        "    # Construct the full path to the file.\n",
        "    pdf_path = dir + pdf_name\n",
        "\n",
        "    # Construct the file name for the output by adding the tage \" - Parsed\" to\n",
        "    # the end of the filename and replacing the file extension '.pdf' with\n",
        "    # '.txt'.\n",
        "    text_file_path = pdf_path[0:-4] + \" - Parsed.txt\"\n",
        "\n",
        "    # If the output .txt file already exists, then I'm assuming we already took\n",
        "    # care of it, so skip this PDF.\n",
        "    if os.path.exists(text_file_path):\n",
        "        print('Skipping - Already Parsed.')\n",
        "        continue\n",
        "\n",
        "    # Track the time.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # ======== Open ========\n",
        "\n",
        "    # Open the PDF file as a fitz.Document\n",
        "    doc = fitz.open(pdf_path)\n",
        "\n",
        "\n",
        "    # ======== Try Text ========\n",
        "    all_text = \"\"\n",
        "\n",
        "    # For each page in the document...\n",
        "    for i, page in enumerate(doc):\n",
        "\n",
        "        # Retrieve whatever text exists.\n",
        "        all_text += page.get_text()\n",
        "\n",
        "    # If there wasn't any text...\n",
        "    if len(all_text) == 0:\n",
        "\n",
        "        # ======== Run OCR =========\n",
        "        # We'll try parsing the images.\n",
        "\n",
        "        print('  Running OCR...')\n",
        "\n",
        "        # For each page...\n",
        "        for i, page in enumerate(doc):\n",
        "\n",
        "            # This can be slow, so we'll print out the current page number to\n",
        "            # show our progress.\n",
        "            print('    Page {:} of {:}'.format(i + 1, len(doc)))\n",
        "\n",
        "            # Make the `TextPage` object using the `get_textpage_ocr` function.\n",
        "            # This is the step that does all of the OCR.\n",
        "            full_tp = page.get_textpage_ocr(flags=0, dpi=300, full=True)\n",
        "\n",
        "            # Next, we'll pass this `full_tp` to the `get_text` function to\n",
        "            # pull out the text.\n",
        "            #\n",
        "            # By passing the \"blocks\" argument, we'll get a list of text blocks\n",
        "            # from the page. I'm guessing this means the library first\n",
        "            # identifies regions of text in the image, and then runs OCR on\n",
        "            # them separately.\n",
        "            #\n",
        "            # Some example code from the library's GitHub repo showed this\n",
        "            # approach, where we are eliminating *all* newline characters from\n",
        "            # the text. I followed this example because, wihtout it, there does\n",
        "            # seem to be a ton of whitespace! ChatGPT won't care, but if you\n",
        "            # want to read the parsed file yourself, it helps to clean that up.\n",
        "\n",
        "            # Get the parsed text blocks.\n",
        "            blocks = page.get_text(\"blocks\", textpage=full_tp)\n",
        "\n",
        "            # For each block...\n",
        "            for b in blocks:\n",
        "                # The first four elements of 'b' appear to be something like\n",
        "                # coordinates, and the fifth element, b[4], is the actual text.\n",
        "                #\n",
        "                # Replace *all* of the newline characters with a single space,\n",
        "                # but then add back in a single newline character at the end\n",
        "                # to separate the blocks.\n",
        "                all_text += b[4].replace(\"\\n\", \" \") + '\\n'\n",
        "\n",
        "            # Alternatively, if you ommit the \"blocks\" argument, you'll just\n",
        "            # get a single string with all of the text.\n",
        "            #all_text += page.get_text(textpage=full_tp)\n",
        "\n",
        "    # ======== Record to Disk ========\n",
        "    print('  Writing out scanned text to:\\n    ', text_file_path)\n",
        "\n",
        "    # Write all of the text to the .txt file.\n",
        "    with open(text_file_path, \"w\") as f:\n",
        "        f.write(all_text)\n",
        "\n",
        "    print('  Done.')\n",
        "\n",
        "    print('  Elapsed:', format_time(time.time() - t0))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9452cc52-e9e7-4f92-caa6-9aae698fe151",
        "id": "4gkEGuzDeW0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== America and China Cooperating on Climate.pdf (1 of 3) ========\n",
            "\n",
            "  Writing out scanned text to:\n",
            "     ./drive/MyDrive/Readings/America and China Cooperating on Climate - Parsed.txt\n",
            "  Done.\n",
            "  Elapsed: 0:00:01\n",
            "\n",
            "======== Machiavelli - The Prince - Chp 18.pdf (2 of 3) ========\n",
            "\n",
            "  Writing out scanned text to:\n",
            "     ./drive/MyDrive/Readings/Machiavelli - The Prince - Chp 18 - Parsed.txt\n",
            "  Done.\n",
            "  Elapsed: 0:00:00\n",
            "\n",
            "======== Machiavelli - The Prince - Chp 19.pdf (3 of 3) ========\n",
            "\n",
            "  Writing out scanned text to:\n",
            "     ./drive/MyDrive/Readings/Machiavelli - The Prince - Chp 19 - Parsed.txt\n",
            "  Done.\n",
            "  Elapsed: 0:00:00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#▂▂▂▂▂▂▂▂▂▂▂▂"
      ],
      "metadata": {
        "id": "yyzkaC95i81U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2 - Summarizing"
      ],
      "metadata": {
        "id": "32UOpueYi81U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can finally summarize the articles / chapters!"
      ],
      "metadata": {
        "id": "U0zSyLoFqfdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. The Problem with Long Text"
      ],
      "metadata": {
        "id": "QqfJSbGmx5L2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarizing Long Passages**\n",
        "\n",
        "A significant limitation with how ChatGPT is implemented is that it can only accept up to a certain amount of text.\n",
        "\n",
        "If our article is short enough, then asking ChatGPT to summarize it is as simple as something like \"Please summarize the following article: \".\n",
        "\n",
        "But the articles and book chapters assigned in my friend's college class are (usually) too long to be summarized in one go.\n",
        "\n",
        "Instead, we need a strategy for breaking the article into smaller chunks..."
      ],
      "metadata": {
        "id": "jV14gyerHWdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ChatGPT's Conversation \"Memory\"**\n",
        "\n",
        "When you go to chat.openai.com and have a conversation with ChatGPT, you can chat back and forth with it endlessly, and it remembers what's already been said so far in the conversation.\n",
        "\n",
        "It's a little deceptive, though... Once your chat gets really long, it actually starts to _completely_ forget anything beyond the past few thousand words of chat history.\n",
        "\n",
        "ChatGPT can process a maximum of 4,096 \"tokens\" at a time (you could think of this as roughly 3,000 words, or a four page article).\n",
        "\n",
        "Note: This limit may have increased since the time of writing.\n",
        "\n",
        "But this isn't the limit on just the size of the next message you send... The **combined length** of **everything** needs to be short enough. That is, all of the following together must be under this 4,096 token limit:\n",
        "\n",
        "1. _The whole chat history_\n",
        "2. Your next message\n",
        "3. ChatGPT's next reply\n",
        "\n",
        "This is because ChatGPT _doesn't_ actually have a memory of your conversation--it just **re-reads** the **whole conversation** every time it replies!\n",
        "\n",
        "Once you go past the 4,096 token limit, OpenAI starts to **drop** the **oldest parts** of your conversation (behind the scenes) in order to make room for new dialogue.\n"
      ],
      "metadata": {
        "id": "TKe-B2uFIpGz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aside: Why is it limited?**\n",
        "\n",
        "One of the biggest limitations with how ChatGPT is implemented is that the amount of GPU power required to run ChatGPT grows **exponentially** with how long your text is.\n",
        "\n",
        "This leads to a practical problem around how much **time** and **money** it costs to **train** ChatGPT. OpenAI had a big budget to work with, and of course wanted this token limit to be as large as possible, but they still had to pick a cut-off point somewhere.\n",
        "\n",
        "At 4,096 tokens, ChatGPT cost them over a **million dollars** worth of GPU compute power to train. When you take into account that training these models required a lot of **trial-and-error** experimentation by the researchers (i.e., they trained it **many times** in different ways to try and improve its performance!), I imagine it was more like 10s or even 100s of millions of dollars before they arrived at the final version. Bigger models also require the researchers to **wait longer** before they get to see the results of their experiment.\n",
        "\n",
        "Finally, **4,096** seems a little odd... Why not just 4,000? Us humans use the decimal system and tend to like numbers that are powers of 10 (10, 100, 1000, ...). But computers use the \"binary\" system, and so they like numbers that are powers of 2. (2, 4, 8, 16, 32, ...). The number 4,096 is 2 raised to the 12th power.\n"
      ],
      "metadata": {
        "id": "u1jFPdM6IvrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Working Around the Limit**\n",
        "\n",
        "To give an example, one of the reading materials was a 25-page book chapter which needed to be split into 4 chunks in order for each chunk to fit in this limit.\n",
        "\n",
        "But each time I feed it a new big chunk, it has _no memory_ of the previous chunk(s). So that's where we have to get a little creative!\n",
        "\n"
      ],
      "metadata": {
        "id": "sJsPrfHAT5ij"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. My Approach"
      ],
      "metadata": {
        "id": "7jbxJQh3gbT0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\"Prompt Design\"**"
      ],
      "metadata": {
        "id": "ugwdQMydx_YV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Prompt Design\" or \"Prompt Engineering\" refers to figuring out how to phrase and format your request to ChatGPT that will give you the best result.\n",
        "\n",
        "I tried a few different approaches to this summarization task, and also tried optimizing my approach by making tweaks to the phrasing and layout of the prompt.  \n",
        "\n",
        "What I landed on seems to work well, but I bet there's still room for improvement if you want to experiment more!"
      ],
      "metadata": {
        "id": "8I8HktthyA6G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**My Prompt**\n",
        "\n",
        "The trick I'm using to solve this length problem is to provide ChatGPT with a **summary of the previous chunks** of text to give it context as it works on summarizing the current chunk.\n",
        "\n",
        "For each new chunk, I actually break it into two steps:\n"
      ],
      "metadata": {
        "id": "GavGK_U3f1ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 1:** Provide a summary of the past chunks, plus the new chunk. Ask ChatGPT to create a summary of the new chunk.\n",
        "\n",
        "Here's the actual prompt. I'm using curly brackets to denote the variables.\n"
      ],
      "metadata": {
        "id": "CXts_kv8h3zP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        ">>\n",
        "I am working on creating a summary of a long article or book chapter\n",
        "which I have broken into **{total chunks}** segments.\n",
        ">>\n",
        "For context, here is a summary of the first **{# of chunks so far}** segments:\n",
        ">>\n",
        "**{Summary of all chunks so far}**\n",
        ">>\n",
        "Please write a bullet point summary of this next segment in 250 words\n",
        "or less:\n",
        ">>\n",
        "**{next chunk}**\n",
        "\n"
      ],
      "metadata": {
        "id": "r2EbHxVBi5jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 2:** Update the summary by providing it with all of the separate chunk summaries, and asking it to produce a single, more concise version."
      ],
      "metadata": {
        "id": "Krs4640yh46s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">>\n",
        "I am working on creating a summary of a long article or book chapter which I\n",
        "have broken into **{total chunks}** segments.\n",
        ">>\n",
        "Please consolidate the following summaries of the first **{# of chunks so far}** segments down into a single bullet point summary that is 250 words or less:\n",
        ">>\n",
        "Summary of Segment 1:\n",
        ">>\n",
        "**{chunk 1 summary}**\n",
        ">>\n",
        "Summary of Segment 2:\n",
        ">>\n",
        "**{chunk 2 summary}**\n",
        ">>\n",
        "..."
      ],
      "metadata": {
        "id": "2v4BCwWei7OV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code in this Notebook will also create a .txt log file (one log file per article) with all of the actual messages and replies sent and received, so you can read through one of those as an example as well.\n",
        "\n",
        "Now let's move on to the code!"
      ],
      "metadata": {
        "id": "-Urx9O-jgZOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Setup OpenAI"
      ],
      "metadata": {
        "id": "ii6nKemRvhq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we have a bit of setup work to do."
      ],
      "metadata": {
        "id": "yUSsLvr3aKP_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OpenAI**"
      ],
      "metadata": {
        "id": "S7IJtCKXkTSd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This library will allow us to interact with ChatGPT programmatically."
      ],
      "metadata": {
        "id": "teFCytVx6jEO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSQb2ATn677-",
        "outputId": "9d5552d6-05ec-4a18-e27e-d409a9b221e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.9.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**API Key**\n",
        "\n",
        "You'll need to register for an OpenAI account, and provide your API key here in order to use this library.\n",
        "\n",
        "You could modify my code to use more of a copy-and-paste method to work with the free chat interface at chat.openai.com, but that would be pretty slow-going!"
      ],
      "metadata": {
        "id": "MDvMe6tZwaDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the `secrets` feature in Colab (see the key-shaped icon in the sidebar) to allow me to run this Notebook easily without running the risk of accidentally sharing my API key with you. 😜"
      ],
      "metadata": {
        "id": "m48JqOLyVSMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "api_key = userdata.get('Chris_API_key')"
      ],
      "metadata": {
        "id": "cuBYDG5rVodg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pass your API key to the `openai` library."
      ],
      "metadata": {
        "id": "yZWDnPWNVqHX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set up the OpenAI API client\n",
        "openai.api_key = api_key"
      ],
      "metadata": {
        "id": "PS7xx-jr7Cy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Interaction**\n",
        "\n",
        "This cell demonstrates the most basic way to send a prompt to ChatGPT and retrieve its reply.\n",
        "\n",
        "The code is based on an example in the documentation, [here](https://github.com/openai/openai-python?tab=readme-ov-file#module-level-client).\n",
        "\n",
        "The interface is clearly much more feature-rich than just what I'm doing here, but I won't be getting into the details of the API."
      ],
      "metadata": {
        "id": "V9ha2aL1s0Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "prompt = \"Tell me a joke.\"\n",
        "\n",
        "# Send our message to ChatGPT\n",
        "completion = openai.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": prompt,\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "\n",
        "# Extract the reply text.\n",
        "reply = completion.choices[0].message.content\n",
        "\n",
        "# Print the reply\n",
        "print(reply)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0G4Na6ErP1G",
        "outputId": "69a8b8ae-268f-48d9-dd8f-7e672df4f67b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a classic one for you: Why don't scientists trust atoms?\n",
            "Because they make up everything!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Load Tokenizer"
      ],
      "metadata": {
        "id": "lVHACgUOlRWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to break the article down into separate chunks that are less than 4,096 tokens, but how do we know where to make those breaks in the text?\n",
        "\n",
        "Luckily, OpenAI provides the `tiktoken` library which makes this task pretty easy! It will break the article down for us into a list of tokens, and then we can break that list down into chunks.\n",
        "\n",
        "First, install the library."
      ],
      "metadata": {
        "id": "RKhXY9p6jNhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade tiktoken"
      ],
      "metadata": {
        "id": "ypBja2lOq-ts",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f00ad7ef-85dd-41a0-f660-e55e8db45941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the Tokenizer**"
      ],
      "metadata": {
        "id": "nUw7epyPbJkW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ChatGPT isn't the only GPT \"model\" that OpenAI has, and sometimes different models use a different \"vocabulary\" of tokens.\n",
        "\n",
        "To tokenize our text, we need to load the tokenizer for ChatGPT, and we can use the function `encoding_for_model` to have it pick the correct one for us based on our model name. ChatGPT is actually built on version 3.5 of GPT, so that's what you'll see in the code below.\n",
        "\n",
        "Finally, you'll see that we use the word 'encoder' rather than 'tokenizer'. This is a little technicality that you don't need to concern yourself with.\n",
        "\n",
        "But if you're curious... Each token in the vocabulary is represented by an integer, and the `encode` function breaks a text string into a list of token integers (rather than a list of token strings)."
      ],
      "metadata": {
        "id": "OOY7ST5wbOan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# Use tiktoken.encoding_for_model() to automatically load the correct tokenizer\n",
        "# for a given model name.\n",
        "encoder = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "Tt00EBNtjPVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a quick example:"
      ],
      "metadata": {
        "id": "vcGE4VZXdf1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"tiktoken is great!\"\n",
        "\n",
        "# Turn text into tokens with encoding.encode(), which converts the string into a\n",
        "# list of integers.\n",
        "token_ids = encoder.encode(text)\n",
        "\n",
        "print(text, \"-->\")\n",
        "print(token_ids)\n",
        "\n",
        "# It will print: [83, 1609, 5963, 374, 2294, 0]\n",
        "\n",
        "# The length of the list tells us how many tokens are required to represent our\n",
        "# text.\n",
        "num_tokens = len(token_ids)\n",
        "\n",
        "print(\"\\nThe text string breaks into\", num_tokens, \"tokens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbMKZrZ1cck-",
        "outputId": "54d6bbf1-eabe-4f47-e44a-d817c8a33bc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tiktoken is great! -->\n",
            "[83, 1609, 5963, 374, 2294, 0]\n",
            "\n",
            "The text string breaks into 6 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Turning the full text into a list of tokens will allow us to break the list  down into separate segments.\n",
        "\n",
        "But what we need to actually give ChatGPT is a block of text, not a list of numbers!\n",
        "\n",
        "For this, we'll use the `decode` function of the tokenizer."
      ],
      "metadata": {
        "id": "6X-OaeUWsYoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert back from token numbers into a string.\n",
        "# Note that the 'decode' function is able to exactly reproduce the\n",
        "# input string--nothing is lost from encoding and decoding.\n",
        "str_chunk = encoder.decode(token_ids)\n",
        "\n",
        "print(\"The reconstructed string:\")\n",
        "print(str_chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52pGelLYs0p0",
        "outputId": "6a045d87-5a73-48be-8370-0a4b0ed116d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The reconstructed string:\n",
            "tiktoken is great!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5. Summarize!"
      ],
      "metadata": {
        "id": "av2D2ECm1HtH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section contains the code to actually perform the summarization."
      ],
      "metadata": {
        "id": "vd_PYOn0twFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Output Files**\n",
        "\n",
        "For each article, it will produce two .txt files:\n",
        "\n",
        "1. \"`{article name} - Summary.txt`\" - This contains the final summary of the article. I also added two other things that I found useful:\n",
        "    1. I asked ChatGPT to generate a glossary of key terms (with definitions).\n",
        "    2. I included the list of separate chunk summaries, which provides a longer, more detailed overview of the article.\n",
        "\n",
        "2. \"`{article name} - Chat Log.txt`\" - This includes the full text of all messages sent and received. If ChatGPT produces a particularly bad summary for an article, you can read through this log to see if you can figure out why.\n"
      ],
      "metadata": {
        "id": "APUq8b_koKRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Restarting or Resuming**\n",
        "\n",
        "Note that the code will check to see if the \"- Summary.txt\" file already exists, and skip that article if it does. This way, if something goes wrong, you can run the code again and it won't re-do articles it's already finished.\n",
        "\n",
        "If you _do_ want to re-run the code for a particular article (or all of them), just delete the \"- Summary.txt\" and \"- Chat Log.txt\" files for the ones you want to re-run.\n"
      ],
      "metadata": {
        "id": "H7F7YQBpoLs1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunk Size**\n",
        "\n",
        "We can't just break the article into 4,096 token chunks--we need to leave room for the prompt, summary of past chunks, and for ChatGPT's reply.\n",
        "\n",
        "There's no way to predict how much room we'll need for those, but breaking the article down into 3,000 token chunks seems to be a safe bet. You can try adjusting this number if you want.\n"
      ],
      "metadata": {
        "id": "N16_waOCq61R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many tokens in each chunk.\n",
        "chunk_size = 3000"
      ],
      "metadata": {
        "id": "IGX6MmssrECr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**List of Articles**\n",
        "\n",
        "Get the list of all of the parsed articles that we are going to summarize."
      ],
      "metadata": {
        "id": "27kjftwB1HtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "txt_filenames = []\n",
        "\n",
        "#dir = \"./drive/MyDrive/Readings/\"\n",
        "\n",
        "files = os.listdir(dir)\n",
        "\n",
        "for f in files:\n",
        "\n",
        "    # Filter for pdfs.\n",
        "    if ' - Parsed.txt' in f:\n",
        "\n",
        "        # Add to the list\n",
        "        txt_filenames.append(f)\n",
        "\n",
        "        print(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b53412ee-32f6-40f5-e041-cb57eec06375",
        "id": "BHPPSKro1HtI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "America and China Cooperating on Climate - Parsed.txt\n",
            "Machiavelli - The Prince - Chp 18 - Parsed.txt\n",
            "Machiavelli - The Prince - Chp 19 - Parsed.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summarization Loop**\n",
        "\n",
        "Let's goooo!"
      ],
      "metadata": {
        "id": "xgy20nSNrIT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "# This tiktoken 'Encoding' object can tokenize and de-tokenize our text.\n",
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# When we print out the final summary, we'll wrap text to 80 characters.\n",
        "wrapper = textwrap.TextWrapper(width=80)\n",
        "\n",
        "# For each of the documents...\n",
        "for (txt_num, txt_name) in enumerate(txt_filenames):\n",
        "\n",
        "    # We already filtered for this, but just to sanity check, ensure we're only\n",
        "    # running this on the \"Parsed\" files, and not the summaries. :-P\n",
        "    assert(\" - Parsed.txt\" in txt_name)\n",
        "\n",
        "    # Get the base filename.\n",
        "    base_name = txt_name[0:-len(\" - Parsed.txt\")]\n",
        "\n",
        "    # We'll be writing out two more files--the final summary, and a log of\n",
        "    # the actual chat. (Note: I ommit the article chunks)\n",
        "    summary_name = base_name + \" - Summary.txt\"\n",
        "    log_name = base_name + \" - Chat Log.txt\"\n",
        "\n",
        "    # Print the name of the article we're about to work on.\n",
        "    print('\\n======== {:} ({:} of {:}) ========\\n'.format(base_name, txt_num + 1, len(txt_filenames)) )\n",
        "\n",
        "    # If the summary output already exists...\n",
        "    if os.path.exists(dir + summary_name):\n",
        "         print('  Skipping - Already Summarized.')\n",
        "         continue\n",
        "\n",
        "    # Track the time.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Read in the full text of the article.\n",
        "    with open(dir + txt_name, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    # Combine all of the lines into a single string.\n",
        "    text = \"\".join(lines)\n",
        "\n",
        "    # ======== Tokenize ========\n",
        "\n",
        "    # Using tiktoken, we can split our string into ChatGPT tokens. Each token\n",
        "    # is represented here by its index in the vocabulary.\n",
        "    tokens = encoding.encode(text)\n",
        "\n",
        "    # Determine the number of chunks we'll need to break this into.\n",
        "    chunk_count = float(len(tokens)) / float(chunk_size)\n",
        "\n",
        "    # Round up.\n",
        "    chunk_count = int(np.ceil(chunk_count))\n",
        "\n",
        "    # Print out the full token count of the article, and how many chunks we'll\n",
        "    # be breaking it into.\n",
        "    print(\"  Token count: {:,}   ({:} chunks of {:,} tokens)\".format(len(tokens), chunk_count, chunk_size))\n",
        "\n",
        "    # ======== Split Chunks ========\n",
        "\n",
        "    str_chunks = []\n",
        "\n",
        "    # For each chunk of tokens...\n",
        "    for i in range(0, len(tokens), chunk_size):\n",
        "\n",
        "        # Get the next cunk of tokens.\n",
        "        # (Note that Python is nice enough to automatically handle the end of\n",
        "        # the array for us.)\n",
        "        token_chunk = tokens[i:i + chunk_size]\n",
        "\n",
        "        # Convert back from token numbers into a string.\n",
        "        # Note that the 'decode' function is able to exactly reproduce the\n",
        "        # input string--nothing is lost from encoding and decoding.\n",
        "        str_chunk = encoding.decode(token_chunk)\n",
        "\n",
        "        # If the chunks line up such that the last one is tiny, then skip it.\n",
        "        # I figure too small of an input might do more harm than good to the\n",
        "        # summary--but I haven't tested that theory!\n",
        "        if len(str_chunk) < 1000:\n",
        "            print('Dropping the last chunk -- only {:} characters'.format(len(str_chunk)))\n",
        "            continue\n",
        "\n",
        "        str_chunks.append(str_chunk)\n",
        "\n",
        "    num_chunks = len(str_chunks)\n",
        "\n",
        "    # ======== Summarize! ========\n",
        "    with open(dir + log_name, 'w') as f:\n",
        "\n",
        "        print('\\nSummarizing chunks...')\n",
        "\n",
        "        chunk_summaries = []\n",
        "        agg_summary = \"\"\n",
        "        acc_prompt = \"\"\n",
        "\n",
        "        # For each of the chunks...\n",
        "        for i in range(len(str_chunks)):\n",
        "\n",
        "            print('\\n  Chunk {:} of {:}'.format(i + 1, num_chunks))\n",
        "\n",
        "            # ======== Step 1: Summarize Next Chunk ========\n",
        "            # Provide a summary of the past chunks, plus the new chunk.\n",
        "            # Ask ChatGPT to create a summary of the new chunk.\n",
        "\n",
        "            # The first chunk is a special case since we don't have context to\n",
        "            # provide yet.\n",
        "            if i == 0:\n",
        "                # State the problem and what we want.\n",
        "                prompt = \\\n",
        "\"I am working on creating a summary of an article which I have broken into {:} \\\n",
        "segments. Below is the first segment; please write a bullet point summary of \\\n",
        "it in 250 words or less:\\n\\n\".format(num_chunks) + str_chunks[0]\n",
        "\n",
        "            # For all subsequent chunks, we'll provide the summary plus the new\n",
        "            # chunk.\n",
        "            else:\n",
        "                # State the problem.\n",
        "                prompt = \\\n",
        "\"I am working on creating a summary of a long article or book chapter which I \\\n",
        "have broken into {:} segments.\\nFor context, here is a summary of the first \\\n",
        "{:} segments:\\n\\n\".format(num_chunks, i)\n",
        "\n",
        "                # Insert the summary of the prior chunks.\n",
        "                prompt += agg_summary + \"\\n\"\n",
        "\n",
        "                # Ask it to summarize.\n",
        "                prompt += \\\n",
        "\"\\nPlease write a bullet point summary of this next segment in 250 words \\\n",
        "or less:\\n\\n\"\n",
        "                # Add the text for the current chunk.\n",
        "                prompt += str_chunks[i]\n",
        "\n",
        "            # Send it to ChatGPT!\n",
        "            completion = openai.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt,\n",
        "                    },\n",
        "                ],\n",
        "            )\n",
        "\n",
        "            # Extract the reply text.\n",
        "            reply = completion.choices[0].message.content\n",
        "\n",
        "            # Update the list of summaries.\n",
        "            chunk_summaries.append(reply)\n",
        "\n",
        "            # Write the response to the chat log file.\n",
        "            f.write(\"\\n\\n▂▂▂▂▂▂▂▂▂▂▂▂▂ ↓ Prompt (Chunk {:} of {:}) ↓ ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\\n\".format(i + 1, num_chunks))\n",
        "            f.write(prompt)\n",
        "            f.write(\"\\n▂▂▂▂▂▂▂▂▂▂▂▂▂ ↓ Reply (Chunk {:} of {:}) ↓ ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\\n\".format(i + 1, num_chunks))\n",
        "            f.write(reply)\n",
        "\n",
        "            # Report token counts.\n",
        "            num_prompt_tokens = len(encoder.encode(prompt))\n",
        "            num_reply_tokens = len(encoder.encode(reply))\n",
        "            total_tokens = num_prompt_tokens + num_reply_tokens\n",
        "\n",
        "            print('    Token counts - Prompt: {:,}    Reply: {:,}    Total: {:,}  (max is 4,096)'.format(num_prompt_tokens, num_reply_tokens, total_tokens))\n",
        "\n",
        "            # ======== Step 2: Summarize the Summaries ========\n",
        "            # Ask ChatGPT to consolidate all of the existing chunk summaries\n",
        "            # down into a single condensed version.\n",
        "\n",
        "            # For the first chunk, we don't need to ask it to consolidate\n",
        "            # anything.\n",
        "            if i == 0:\n",
        "                agg_summary = reply\n",
        "\n",
        "            # For subsequent chunks, provide each of the separate summaries and\n",
        "            # ask it to create a single, condensed version.\n",
        "            else:\n",
        "                # Explain the task, and what we want.\n",
        "                prompt = \\\n",
        "\"I am working on creating a summary of a long article or book chapter which I \\\n",
        "have broken into {:} segments.\\n Please consolidate the following summaries of \\\n",
        "the first {:} segments down into a single bullet point summary that is 250 \\\n",
        "words or less:\\n\".format(num_chunks, i+1)\n",
        "\n",
        "                # Add each of the separate summaries:\n",
        "                for (s_i, s) in enumerate(chunk_summaries):\n",
        "                    prompt += \"\\nSummary of Segment {:}:\\n\".format(s_i + 1)\n",
        "                    prompt += s\n",
        "\n",
        "                # Send it to ChatGPT!\n",
        "                completion = openai.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[\n",
        "                        {\n",
        "                            \"role\": \"user\",\n",
        "                            \"content\": prompt,\n",
        "                        },\n",
        "                    ],\n",
        "                )\n",
        "\n",
        "                # Extract the reply text.\n",
        "                reply = completion.choices[0].message.content\n",
        "\n",
        "                # The reply is our new consolidated summary of the prior chunks.\n",
        "                agg_summary = reply\n",
        "\n",
        "                # Write the response to the chat log file.\n",
        "                f.write(\"\\n\\n▂▂▂▂▂▂▂▂▂▂▂▂▂ ↓ Prompt (Chunk {:} of {:}) ↓ ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\\n\".format(i + 1, len(str_chunks)))\n",
        "                f.write(prompt)\n",
        "                f.write(\"\\n▂▂▂▂▂▂▂▂▂▂▂▂▂ ↓ Reply (Chunk {:} of {:}) ↓ ▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂\\n\".format(i + 1, len(str_chunks)))\n",
        "                f.write(reply)\n",
        "\n",
        "            # End of the chunk loop!\n",
        "\n",
        "    # At this point, we have summaries of all of the segments, and a single,\n",
        "    # final summary.\n",
        "\n",
        "    # Print the final summary! This uses the wrapper tool (instantiated at the\n",
        "    # top of this cell) to wrap the text to 80-characters wide, for readability.\n",
        "    print('\\nFinal, overall summary:\\n')\n",
        "    print('--------')\n",
        "    print(wrapper.fill(agg_summary))\n",
        "    print('--------')\n",
        "    print('')\n",
        "\n",
        "    # ======== Glossary ========\n",
        "    # As a final step, let's ask for a glossary of key terms!\n",
        "\n",
        "    # Explain the task and what we want.\n",
        "    prompt = \\\n",
        "\"The following is a summary of a chapter of a book. Can you create a glossary \\\n",
        "for the key terms that readers might not be familiar with?\\n\\n\" + agg_summary\n",
        "\n",
        "    # Send it to chat gpt!\n",
        "    completion = openai.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Extract the reply text.\n",
        "    reply = completion.choices[0].message.content\n",
        "\n",
        "    # The reply should be a glossary of key terms.\n",
        "    glossary = reply\n",
        "\n",
        "    # Save the final summary to a text file.\n",
        "    with open(dir + summary_name, 'w') as f:\n",
        "\n",
        "        f.write(\"This document contains:\\n\")\n",
        "        f.write(\"  1. The overall summary of the document.\\n\")\n",
        "        f.write(\"  2. A glossary of key terms.\\n\")\n",
        "        f.write(\"  3. The separate summaries of each chunk of text that the doc was broken into.\\n\")\n",
        "        f.write(\"\\n\")\n",
        "        f.write(\"▂▂▂▂▂▂▂ Overall Summary ▂▂▂▂▂▂▂\\n\")\n",
        "        f.write(agg_summary)\n",
        "\n",
        "        f.write(\"\\n\\n\")\n",
        "        f.write(\"▂▂▂▂▂▂▂ Glossary of Key Terms ▂▂▂▂▂▂▂\\n\")\n",
        "        f.write(glossary)\n",
        "\n",
        "        # For each separate chunk summary...\n",
        "        for (s_i, s) in enumerate(chunk_summaries):\n",
        "            f.write(\"\\n\\n\")\n",
        "            f.write(\"▂▂▂▂▂▂▂ Summary of Chunk {:} of {:} ▂▂▂▂▂▂▂\\n\".format(s_i + 1, num_chunks))\n",
        "            f.write(s)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628b2e8b-17d7-4250-f6e7-edaa1cf82557",
        "id": "WbfZqmgh1HtI"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== America and China Cooperating on Climate (1 of 3) ========\n",
            "\n",
            "  Token count: 3,602   (2 chunks of 3,000 tokens)\n",
            "\n",
            "Summarizing chunks...\n",
            "\n",
            "  Chunk 1 of 2\n",
            "    Token counts - Prompt: 3,040    Reply: 226    Total: 3,266  (max is 4,096)\n",
            "\n",
            "  Chunk 2 of 2\n",
            "    Token counts - Prompt: 882    Reply: 200    Total: 1,082  (max is 4,096)\n",
            "\n",
            "Final, overall summary:\n",
            "\n",
            "--------\n",
            "The United States and China should establish a shared climate finance platform\n",
            "to provide coordinated financial assistance to developing countries in the fight\n",
            "against climate change. This collaboration would maximize the impact of aid,\n",
            "promote low-carbon economic development, reduce poverty, and enhance resilience\n",
            "to climate change. By working together, they can assist low-income developing\n",
            "countries, currently reliant on fossil fuels, to break free from poverty and\n",
            "debt. Both countries have the necessary infrastructure and resources to support\n",
            "this collaboration, including aid agencies, banking institutions, clean\n",
            "technologies, and technical expertise. Even if true cooperation is not possible,\n",
            "coordinating their climate finance efforts would allow for complementary\n",
            "projects in the same developing country, achieving more collectively. Through\n",
            "co-financing, both countries can achieve more with limited resources, spread\n",
            "financial risk, and gain a better understanding of each other's preferences and\n",
            "approaches to blending public and private finance. Including additional\n",
            "partners, such as a Middle Eastern sovereign wealth fund, would enhance the\n",
            "platform's political acceptability and funding potential. This joint financing\n",
            "platform presents a pragmatic and necessary opportunity for the US and China to\n",
            "pioneer a new approach to financing green development, setting an example for\n",
            "the world and encouraging future collaboration.\n",
            "--------\n",
            "\n",
            "\n",
            "======== Machiavelli - The Prince - Chp 18 (2 of 3) ========\n",
            "\n",
            "  Token count: 1,288   (1 chunks of 3,000 tokens)\n",
            "\n",
            "Summarizing chunks...\n",
            "\n",
            "  Chunk 1 of 1\n",
            "    Token counts - Prompt: 1,328    Reply: 227    Total: 1,555  (max is 4,096)\n",
            "\n",
            "Final, overall summary:\n",
            "\n",
            "--------\n",
            "- Leaders who are honest and keep their word are admirable, but leaders who are\n",
            "cunning and deceitful often achieve more. - There are two ways of doing battle:\n",
            "using the law and using force. A ruler must be able to exploit both approaches.\n",
            "- A ruler must be able to act like a fox and a lion; playing the fox to see\n",
            "snares and the lion to scare off threats. - It is important for a ruler to seem\n",
            "virtuous, even if they are not truly virtuous, in order to maintain power and\n",
            "control. - People judge leaders based on appearances and end results, rather\n",
            "than firsthand experience or their true character. - A ruler must be careful not\n",
            "to say anything that doesn't appear to be inspired by the five virtues:\n",
            "compassion, loyalty, humanity, honesty, and religion. Appearing religious is\n",
            "especially important. - The crowd is easily won over by appearances and final\n",
            "results, and dissenting opinions have no ground when the majority holds their\n",
            "opinions. - There are leaders who preach peace and trust but are actually\n",
            "enemies to both, and their authority and kingdom are at risk if they ever\n",
            "practiced either.\n",
            "--------\n",
            "\n",
            "\n",
            "======== Machiavelli - The Prince - Chp 19 (3 of 3) ========\n",
            "\n",
            "  Token count: 4,953   (2 chunks of 3,000 tokens)\n",
            "\n",
            "Summarizing chunks...\n",
            "\n",
            "  Chunk 1 of 2\n",
            "    Token counts - Prompt: 3,040    Reply: 309    Total: 3,349  (max is 4,096)\n",
            "\n",
            "  Chunk 2 of 2\n",
            "    Token counts - Prompt: 2,317    Reply: 269    Total: 2,586  (max is 4,096)\n",
            "\n",
            "Final, overall summary:\n",
            "\n",
            "--------\n",
            "A ruler must avoid behavior that leads to hatred or contempt, such as seizing\n",
            "property and women, being seen as changeable or indecisive, and lacking\n",
            "seriousness or strength. To prevent conspiracies, a ruler should be well-\n",
            "respected by his people and have their support. External threats can be\n",
            "addressed with a good army and allies, while internal threats require avoiding\n",
            "despisal from subjects. The greed of the army can also be a hazard, so a ruler\n",
            "must hold both soldiers and people in check. It is important for a ruler to\n",
            "avoid the country's overall hatred and focus on gaining favor with the most\n",
            "powerful classes. The examples of Severus, Antoninus, Commodus, and Maximinus\n",
            "illustrate the effects of respect, hatred, and cruelty on their fates. While\n",
            "contemporary rulers do not prioritize satisfying the army like Roman emperors\n",
            "did, Turkish and Egyptian sultans must prioritize their goodwill for security.\n",
            "Egypt is unique with an elected ruler and well-established institutions. Hatred\n",
            "and contempt led to the downfall of Roman emperors, and a successful ruler\n",
            "should take policies from Severus to found a state and from Marcus Aurelius to\n",
            "bring stability and glory.\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#▂▂▂▂▂▂▂▂▂▂▂▂"
      ],
      "metadata": {
        "id": "MG4baIky_G7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix"
      ],
      "metadata": {
        "id": "-PpYkYr_r9i-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.1. PDF Splitter"
      ],
      "metadata": {
        "id": "W-Cn9fYUGQoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When trying to summarize a really long document like a book, for example, it's probably more helpful to create summaries for each of the individual chapters.\n",
        "\n",
        "The code in this section will help you split a single large PDF into smaller ones (e.g., one PDF per chapter).\n",
        "\n",
        "Once the PDF is split, you can run this Notebook on the splits (rather than the original PDF) in order to get separate summaries."
      ],
      "metadata": {
        "id": "fQDe_FvM25N6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inputs**\n",
        "\n",
        "Provide the path to the document."
      ],
      "metadata": {
        "id": "HBVP21HxLtSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir = './drive/MyDrive/Readings/Split-PDFs/'\n",
        "\n",
        "filename = 'Machiavelli - The Prince.pdf'\n",
        "\n",
        "# Remove the '.pdf' extension to get the base name.\n",
        "base_name = filename[0:-4]"
      ],
      "metadata": {
        "id": "IklFhiDyLvaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill out the list with the first page of each split."
      ],
      "metadata": {
        "id": "PVspaixKGSxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This divide the PDF into 10 separate PDFs. The first will contain pages 1-22,\n",
        "# the second will contain pages 23-51, and so on.\n",
        "#split_starts = [1, 23, 52, 73, 98, 123, 139, 163, 192, 205]\n",
        "\n",
        "# Pg 127 is the start of chapter 18, pg 130 is chapter 19, pg 140 is chapter 20.\n",
        "split_starts = [1, 127, 130, 140]\n",
        "\n",
        "final_page = -1"
      ],
      "metadata": {
        "id": "m3kbE35hGYkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**From Table of Contents**\n",
        "\n",
        "This little block of code (which is disabled) is helpful for splitting books or documents that have a table of contents.\n",
        "\n",
        "1. Copy the page numbers from the table of contents into the `book_pages` list.\n",
        "2. Calculate the offset. Which PDF page is page 1 of the book? Subtract one from that page number to get the offset.\n",
        "\n",
        "The code simply adds the offset to the page numbers for you."
      ],
      "metadata": {
        "id": "Wfbeb5ru06OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map, e.g., from a book's table of contents to page numbers within the PDF.\n",
        "# In the below example, Chapter 1 of the book is on page 1 of the book, but page\n",
        "# 23 of the PDF. Chapter is on pg. 30 of the book, but page 52 of the PDF. And\n",
        "# so on.\n",
        "if False:\n",
        "    # Page numbers according to the book.\n",
        "    book_pages = [1, 30, 51, 76, 101, 117, 141, 170, 183]\n",
        "\n",
        "    # Page one of the book occurs on page 23 of the PDF, so the offset is 22.\n",
        "    pdf_offset = 22\n",
        "\n",
        "    # The first split will contain pages 1-22 of the PDF.\n",
        "    pdf_splits = [1]\n",
        "\n",
        "    # Add the PDF offset to each of the book page numbers.\n",
        "    for bp in book_pages:\n",
        "        pdf_pages.append(bp + pdf_offset)\n",
        "\n",
        "    split_starts = pdf_pages\n",
        "\n",
        "    print(pdf_pages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFn9FGPpGgv4",
        "outputId": "65472a91-2d67-4a5f-dc48-9248ca243063"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 23, 52, 73, 98, 123, 139, 163, 192, 205]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create the Splits**\n",
        "\n",
        "The resulting split files will be named 'Example Book - 1.pdf', 'Example Book - 2.pdf', and so on."
      ],
      "metadata": {
        "id": "KZx5CStv2gLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, fitz\n",
        "\n",
        "with fitz.open(dir + filename) as src_doc:\n",
        "\n",
        "    print('Splitting', dir + filename)\n",
        "\n",
        "    # Set the ending page of the last split to be the last page of the PDF, if\n",
        "    # not specified.\n",
        "    if final_page == -1:\n",
        "        final_page = len(src_doc)\n",
        "        print('Final page =', final_page)\n",
        "\n",
        "    # For each of the splits...\n",
        "    for (split_i, start_page) in enumerate(split_starts):\n",
        "\n",
        "        # For the purposes of printing progress and naming the files, number the\n",
        "        # splits starting from 1.\n",
        "        split_num = split_i + 1\n",
        "        print('  Split', split_num)\n",
        "\n",
        "        # Determine what should be the last page of the current split.\n",
        "\n",
        "        # If it's the final split, use the 'final_page' variable.\n",
        "        if split_num == len(split_starts):\n",
        "            end_page = final_page\n",
        "\n",
        "        # Otherwise, use one less than the start of the next split.\n",
        "        else:\n",
        "            end_page = split_starts[split_i + 1] - 1\n",
        "\n",
        "        # Create a new PDF object for this split.\n",
        "        split_doc = fitz.open()\n",
        "\n",
        "        # Add the pages from the current split.\n",
        "        split_doc.insert_pdf(\n",
        "            src_doc, # The original doc\n",
        "            from_page = start_page - 1, # 0-indexed, so subtract 1.\n",
        "            to_page = end_page - 1,\n",
        "        )\n",
        "\n",
        "        # Write out the new PDF, adding in the split number.\n",
        "        split_doc.save(dir + base_name + ' - {:}.pdf'.format(split_num))\n",
        "\n",
        "        split_doc.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c5hW0TJHEwB",
        "outputId": "827a0790-a52a-4e52-e34e-a0cb6f3bbc37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting ./drive/MyDrive/Readings/Split-PDFs/Machiavelli - The Prince.pdf\n",
            "  Split 1\n",
            "  Split 2\n",
            "  Split 3\n",
            "  Split 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.2. PDF Rotator"
      ],
      "metadata": {
        "id": "RAgdzsTQTplL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the assigned readings was a scan of a book chapter, but the PDF pages showed the book rotated 90 degrees. In order to parse and summarize this PDF, we first needed to fix the page rotation using the below code."
      ],
      "metadata": {
        "id": "oRPsKGCWZ0Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "input_pdf_path = \"./drive/MyDrive/Readings/Example Article.pdf\"\n",
        "output_pdf_path = \"./drive/MyDrive/Readings/Example Article - Rotated.pdf\"\n",
        "\n",
        "# Open the PDF file\n",
        "pdf_document = fitz.open(input_pdf_path)\n",
        "\n",
        "# Iterate through each page and rotate it 90 degrees counter-clockwise\n",
        "for page_num in range(pdf_document.page_count):\n",
        "    page = pdf_document[page_num]\n",
        "    page.set_rotation(-90)  # Rotate 90 degrees counter-clockwise\n",
        "\n",
        "# Save the modified PDF to a new file\n",
        "pdf_document.save(output_pdf_path)\n",
        "pdf_document.close()\n"
      ],
      "metadata": {
        "id": "uSEqdiy3TsEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A.3. Extract a Webpage"
      ],
      "metadata": {
        "id": "nqbqi-3-xXnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is some code for running this Notebook on HTML files instead of PDFs.\n",
        "\n",
        "The \"input\" to Part 2 of the Notebook (which does the summarizing) is that it just runs on any `* - Parsed.txt` files it finds in the specified directory, so really as long as you can get your documents into that format, you're good to go!\n",
        "\n",
        "The below code extracts all of the plain text from any `.html` files in the specified folder and writes out the corresponding ` - Parsed.txt` files. So you can run this section and then go run Part 2."
      ],
      "metadata": {
        "id": "_lwmhnCvYYCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Beautiful Soup - HTML Parsing**"
      ],
      "metadata": {
        "id": "hOBbPuGmxei9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBKGuRRXxgxC",
        "outputId": "b995eb22-f5bd-48c2-f1ae-3f08412cc774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dir = \"./drive/MyDrive/Readings/\"\n",
        "\n",
        "files = os.listdir(dir)\n",
        "\n",
        "# We'll construct a list of just paths to the HTML files.\n",
        "html_filenames = []\n",
        "\n",
        "# For each file in the directory...\n",
        "for f in files:\n",
        "\n",
        "    # Filter for HTML.\n",
        "    if '.html' in f:\n",
        "\n",
        "        # Add to the list\n",
        "        html_filenames.append(f)\n",
        "\n",
        "        print(f)"
      ],
      "metadata": {
        "id": "nAYqCU_6xljo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# For each of the documents...\n",
        "for (html_num, html_name) in enumerate(html_filenames):\n",
        "\n",
        "    # Print out which one we're on.\n",
        "    print('\\n======== {:} ({:} of {:}) ========\\n'.format(html_name, html_num + 1, len(html_filenames)) )\n",
        "\n",
        "    # Construct the full path to the file.\n",
        "    html_path = dir + html_name\n",
        "\n",
        "    # Construct the file name for the output by adding the tage \" - Parsed\" to\n",
        "    # the end of the filename and replacing the file extension '.html' with\n",
        "    # '.txt'.\n",
        "    text_file_path = html_path[0:-5] + \" - Parsed.txt\"\n",
        "\n",
        "    with open(dir + html_name, 'r') as f:\n",
        "        # Read the HTML file.\n",
        "        html_content = f.readlines()\n",
        "\n",
        "        # Convert from a list to a single string.\n",
        "        html_content = '\\n'.join(html_content)\n",
        "\n",
        "        # Set up the HTML parser.\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Estract the plain text.\n",
        "        plain_text = soup.get_text()\n",
        "\n",
        "        # Write all of the text to the .txt file.\n",
        "        with open(text_file_path, \"w\") as f:\n",
        "            f.write(all_text)\n",
        "\n"
      ],
      "metadata": {
        "id": "InTWblngxaCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example HTML content\n",
        "html_content = \"\"\"\n",
        "<html>\n",
        "    <head>\n",
        "        <title>Sample HTML</title>\n",
        "    </head>\n",
        "    <body>\n",
        "        <h1>Hello, World!</h1>\n",
        "        <p>This is a sample HTML document.</p>\n",
        "        <ul>\n",
        "            <li>Item 1</li>\n",
        "            <li>Item 2</li>\n",
        "            <li>Item 3</li>\n",
        "        </ul>\n",
        "    </body>\n",
        "</html>\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flV1Ud3mx7B0",
        "outputId": "e87d5918-f7cc-4183-def5-e689d3eed857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Sample HTML\n",
            "\n",
            "\n",
            "Hello, World!\n",
            "This is a sample HTML document.\n",
            "\n",
            "Item 1\n",
            "Item 2\n",
            "Item 3\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ▂▂▂▂▂▂▂▂▂▂▂▂"
      ],
      "metadata": {
        "id": "jZYz2aHPogQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End"
      ],
      "metadata": {
        "id": "OnNS7pOh_Q1Y"
      }
    }
  ]
}